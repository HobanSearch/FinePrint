"""
Model Evaluation Service for Fine Print AI LoRA Training
Business-specific model evaluation with comprehensive metrics
"""

import asyncio
import json
import time
import uuid
from datetime import datetime
from typing import Dict, Any, Optional, List, Tuple
from pathlib import Path

import numpy as np
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from datasets import Dataset
from sklearn.metrics import accuracy_score, f1_score, precision_recall_fscore_support
import evaluate

from sqlalchemy.ext.asyncio import AsyncSession
import redis.asyncio as aioredis

from ..core.config import settings
from ..core.logging_config import get_logger
from ..models.schemas import (
    EvaluationRequest, EvaluationResult, EvaluationStatus,
    BusinessType, DatasetConfig
)

logger = get_logger("evaluation")


class EvaluationService:
    """Model evaluation service with business-specific metrics"""
    
    def __init__(self, db: AsyncSession, redis: aioredis.Redis):
        self.db = db
        self.redis = redis
        self.active_evaluations = {}
        
        # Initialize evaluation metrics
        self.bleu_metric = None
        self.rouge_metric = None
        self.bert_score_metric = None
        
        logger.info("Evaluation Service initialized")
    
    async def initialize(self):
        """Initialize evaluation service"""
        try:\n            # Load evaluation metrics\n            self.bleu_metric = evaluate.load(\"bleu\")\n            self.rouge_metric = evaluate.load(\"rouge\")\n            self.bert_score_metric = evaluate.load(\"bertscore\")\n            \n            logger.info(\"Evaluation metrics loaded successfully\")\n            \n        except Exception as e:\n            logger.error(\"Failed to initialize evaluation service\", error=str(e))\n            raise\n    \n    async def start_evaluation(self, request: EvaluationRequest) -> str:\n        \"\"\"Start model evaluation\"\"\"\n        evaluation_id = str(uuid.uuid4())\n        \n        try:\n            # Create evaluation record\n            evaluation_data = {\n                \"evaluation_id\": evaluation_id,\n                \"model_id\": request.model_id,\n                \"status\": EvaluationStatus.PENDING.value,\n                \"metrics\": request.metrics,\n                \"business_specific_tests\": request.business_specific_tests,\n                \"evaluation_dataset\": request.evaluation_dataset.dict(),\n                \"created_at\": datetime.utcnow().isoformat(),\n                \"progress\": 0.0\n            }\n            \n            # Store in Redis\n            await self.redis.hset(\n                f\"evaluation:{evaluation_id}\",\n                mapping={k: json.dumps(v) for k, v in evaluation_data.items()}\n            )\n            \n            # Store in memory\n            self.active_evaluations[evaluation_id] = evaluation_data\n            \n            # Start evaluation in background\n            asyncio.create_task(self._run_evaluation(evaluation_id, request))\n            \n            logger.info(\n                \"Evaluation started\",\n                evaluation_id=evaluation_id,\n                model_id=request.model_id\n            )\n            \n            return evaluation_id\n            \n        except Exception as e:\n            logger.error(\"Failed to start evaluation\", evaluation_id=evaluation_id, error=str(e))\n            raise\n    \n    async def get_results(self, evaluation_id: str) -> EvaluationResult:\n        \"\"\"Get evaluation results\"\"\"\n        try:\n            # Try memory first\n            if evaluation_id in self.active_evaluations:\n                eval_data = self.active_evaluations[evaluation_id]\n            else:\n                # Try Redis\n                redis_data = await self.redis.hgetall(f\"evaluation:{evaluation_id}\")\n                if not redis_data:\n                    raise ValueError(f\"Evaluation {evaluation_id} not found\")\n                \n                eval_data = {k: json.loads(v) for k, v in redis_data.items()}\n                self.active_evaluations[evaluation_id] = eval_data\n            \n            return EvaluationResult(\n                evaluation_id=eval_data[\"evaluation_id\"],\n                model_id=eval_data[\"model_id\"],\n                status=EvaluationStatus(eval_data[\"status\"]),\n                metrics=eval_data.get(\"results\", {}),\n                business_specific_results=eval_data.get(\"business_specific_results\"),\n                evaluation_dataset=eval_data[\"evaluation_dataset\"][\"data_source\"],\n                timestamp=datetime.fromisoformat(eval_data[\"created_at\"]),\n                duration_seconds=eval_data.get(\"duration_seconds\", 0.0),\n                error_message=eval_data.get(\"error_message\")\n            )\n            \n        except Exception as e:\n            logger.error(\"Failed to get evaluation results\", evaluation_id=evaluation_id, error=str(e))\n            raise\n    \n    async def health_check(self) -> bool:\n        \"\"\"Check evaluation service health\"\"\"\n        try:\n            # Check Redis connectivity\n            await self.redis.ping()\n            \n            # Check if evaluation metrics are loaded\n            if not (self.bleu_metric and self.rouge_metric and self.bert_score_metric):\n                return False\n            \n            return True\n            \n        except Exception as e:\n            logger.error(\"Evaluation service health check failed\", error=str(e))\n            return False\n    \n    async def cleanup(self):\n        \"\"\"Cleanup evaluation service\"\"\"\n        try:\n            # Cancel any running evaluations\n            for eval_id in list(self.active_evaluations.keys()):\n                eval_data = self.active_evaluations[eval_id]\n                if eval_data[\"status\"] == EvaluationStatus.RUNNING.value:\n                    eval_data[\"status\"] = EvaluationStatus.FAILED.value\n                    eval_data[\"error_message\"] = \"Service shutdown\"\n            \n            logger.info(\"Evaluation Service cleaned up\")\n            \n        except Exception as e:\n            logger.error(\"Error during cleanup\", error=str(e))\n    \n    async def _run_evaluation(self, evaluation_id: str, request: EvaluationRequest):\n        \"\"\"Run model evaluation\"\"\"\n        start_time = time.time()\n        \n        try:\n            # Update status to running\n            await self._update_evaluation_status(evaluation_id, EvaluationStatus.RUNNING)\n            \n            # Get model information\n            model_data = await self.redis.hgetall(f\"model:{request.model_id}\")\n            if not model_data:\n                raise ValueError(f\"Model {request.model_id} not found\")\n            \n            model_info = {k: json.loads(v) for k, v in model_data.items()}\n            business_type = BusinessType(model_info[\"business_type\"])\n            \n            # Load evaluation dataset\n            eval_dataset = await self._load_evaluation_dataset(request.evaluation_dataset)\n            \n            # Load model for evaluation\n            model, tokenizer = await self._load_model_for_evaluation(\n                model_info[\"local_path\"],\n                model_info[\"base_model\"]\n            )\n            \n            # Run standard metrics\n            standard_results = await self._evaluate_standard_metrics(\n                model, tokenizer, eval_dataset, request.metrics\n            )\n            \n            # Run business-specific evaluation\n            business_results = {}\n            if request.business_specific_tests:\n                business_results = await self._evaluate_business_specific(\n                    model, tokenizer, eval_dataset, business_type\n                )\n            \n            # Combine results\n            all_results = {**standard_results, **business_results}\n            \n            # Calculate duration\n            duration = time.time() - start_time\n            \n            # Update evaluation with results\n            eval_data = self.active_evaluations[evaluation_id]\n            eval_data.update({\n                \"status\": EvaluationStatus.COMPLETED.value,\n                \"results\": all_results,\n                \"business_specific_results\": business_results,\n                \"duration_seconds\": duration,\n                \"completed_at\": datetime.utcnow().isoformat(),\n                \"progress\": 100.0\n            })\n            \n            # Update Redis\n            await self.redis.hset(\n                f\"evaluation:{evaluation_id}\",\n                mapping={k: json.dumps(v) for k, v in eval_data.items()}\n            )\n            \n            logger.info(\n                \"Evaluation completed\",\n                evaluation_id=evaluation_id,\n                duration_seconds=duration,\n                results=all_results\n            )\n            \n        except Exception as e:\n            # Mark evaluation as failed\n            duration = time.time() - start_time\n            error_msg = str(e)\n            \n            eval_data = self.active_evaluations[evaluation_id]\n            eval_data.update({\n                \"status\": EvaluationStatus.FAILED.value,\n                \"error_message\": error_msg,\n                \"duration_seconds\": duration,\n                \"completed_at\": datetime.utcnow().isoformat()\n            })\n            \n            await self.redis.hset(\n                f\"evaluation:{evaluation_id}\",\n                mapping={k: json.dumps(v) for k, v in eval_data.items()}\n            )\n            \n            logger.error(\"Evaluation failed\", evaluation_id=evaluation_id, error=error_msg)\n    \n    async def _load_evaluation_dataset(self, dataset_config: DatasetConfig) -> Dataset:\n        \"\"\"Load evaluation dataset\"\"\"\n        try:\n            import datasets\n            \n            # Load dataset based on type\n            if dataset_config.dataset_type == \"json\":\n                dataset = datasets.load_dataset(\n                    \"json\",\n                    data_files=dataset_config.data_source,\n                    split=\"train\"\n                )\n            elif dataset_config.dataset_type == \"csv\":\n                dataset = datasets.load_dataset(\n                    \"csv\",\n                    data_files=dataset_config.data_source,\n                    split=\"train\"\n                )\n            else:\n                dataset = datasets.load_dataset(dataset_config.data_source, split=\"train\")\n            \n            # Limit samples if specified\n            if dataset_config.max_samples and len(dataset) > dataset_config.max_samples:\n                dataset = dataset.select(range(dataset_config.max_samples))\n            \n            return dataset\n            \n        except Exception as e:\n            logger.error(\"Failed to load evaluation dataset\", error=str(e))\n            raise\n    \n    async def _load_model_for_evaluation(self, model_path: str, base_model: str) -> Tuple[Any, Any]:\n        \"\"\"Load model and tokenizer for evaluation\"\"\"\n        try:\n            # Load tokenizer\n            tokenizer = AutoTokenizer.from_pretrained(base_model)\n            if tokenizer.pad_token is None:\n                tokenizer.pad_token = tokenizer.eos_token\n            \n            # Load model\n            model = AutoModelForCausalLM.from_pretrained(\n                model_path,\n                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n                device_map=\"auto\" if torch.cuda.is_available() else None,\n                trust_remote_code=True\n            )\n            \n            model.eval()\n            \n            return model, tokenizer\n            \n        except Exception as e:\n            logger.error(\"Failed to load model for evaluation\", error=str(e))\n            raise\n    \n    async def _evaluate_standard_metrics(\n        self,\n        model: Any,\n        tokenizer: Any,\n        dataset: Dataset,\n        metrics: List[str]\n    ) -> Dict[str, float]:\n        \"\"\"Evaluate standard metrics\"\"\"\n        results = {}\n        \n        try:\n            # Generate predictions\n            predictions, references = await self._generate_predictions(\n                model, tokenizer, dataset\n            )\n            \n            # Calculate BLEU score\n            if \"bleu_score\" in metrics and self.bleu_metric:\n                bleu_result = self.bleu_metric.compute(\n                    predictions=predictions,\n                    references=[[ref] for ref in references]\n                )\n                results[\"bleu_score\"] = bleu_result[\"bleu\"]\n            \n            # Calculate ROUGE score\n            if \"rouge_score\" in metrics and self.rouge_metric:\n                rouge_result = self.rouge_metric.compute(\n                    predictions=predictions,\n                    references=references\n                )\n                results[\"rouge_1\"] = rouge_result[\"rouge1\"]\n                results[\"rouge_2\"] = rouge_result[\"rouge2\"]\n                results[\"rouge_l\"] = rouge_result[\"rougeL\"]\n            \n            # Calculate BERTScore\n            if \"bert_score\" in metrics and self.bert_score_metric:\n                bert_result = self.bert_score_metric.compute(\n                    predictions=predictions,\n                    references=references,\n                    lang=\"en\"\n                )\n                results[\"bert_score_precision\"] = np.mean(bert_result[\"precision\"])\n                results[\"bert_score_recall\"] = np.mean(bert_result[\"recall\"])\n                results[\"bert_score_f1\"] = np.mean(bert_result[\"f1\"])\n            \n            # Calculate perplexity\n            if \"perplexity\" in metrics:\n                perplexity = await self._calculate_perplexity(model, tokenizer, dataset)\n                results[\"perplexity\"] = perplexity\n            \n            return results\n            \n        except Exception as e:\n            logger.error(\"Failed to evaluate standard metrics\", error=str(e))\n            return {}\n    \n    async def _evaluate_business_specific(\n        self,\n        model: Any,\n        tokenizer: Any,\n        dataset: Dataset,\n        business_type: BusinessType\n    ) -> Dict[str, Any]:\n        \"\"\"Evaluate business-specific metrics\"\"\"\n        results = {}\n        \n        try:\n            if business_type == BusinessType.LEGAL_ANALYSIS:\n                results = await self._evaluate_legal_analysis(model, tokenizer, dataset)\n            elif business_type == BusinessType.MARKETING_CONTENT:\n                results = await self._evaluate_marketing_content(model, tokenizer, dataset)\n            elif business_type == BusinessType.SALES_COMMUNICATION:\n                results = await self._evaluate_sales_communication(model, tokenizer, dataset)\n            elif business_type == BusinessType.CUSTOMER_SUPPORT:\n                results = await self._evaluate_customer_support(model, tokenizer, dataset)\n            \n            return results\n            \n        except Exception as e:\n            logger.error(\"Failed to evaluate business-specific metrics\", error=str(e))\n            return {}\n    \n    async def _evaluate_legal_analysis(\n        self,\n        model: Any,\n        tokenizer: Any,\n        dataset: Dataset\n    ) -> Dict[str, Any]:\n        \"\"\"Evaluate legal analysis specific metrics\"\"\"\n        results = {}\n        \n        try:\n            # Test pattern detection accuracy\n            pattern_detection_samples = [\n                {\n                    \"input\": \"Analyze this clause: 'We may terminate your account at any time without notice.'\",\n                    \"expected_patterns\": [\"termination_clauses\"]\n                },\n                {\n                    \"input\": \"Review: 'You waive your right to participate in class action lawsuits.'\",\n                    \"expected_patterns\": [\"class_action_waiver\"]\n                },\n                {\n                    \"input\": \"Check: 'We share your data with third-party partners for marketing.'\",\n                    \"expected_patterns\": [\"data_sharing\"]\n                }\n            ]\n            \n            correct_detections = 0\n            total_samples = len(pattern_detection_samples)\n            \n            for sample in pattern_detection_samples:\n                prediction = await self._generate_single_prediction(\n                    model, tokenizer, sample[\"input\"]\n                )\n                \n                # Check if expected patterns are mentioned in prediction\n                patterns_found = 0\n                for pattern in sample[\"expected_patterns\"]:\n                    if pattern.replace(\"_\", \" \") in prediction.lower():\n                        patterns_found += 1\n                \n                if patterns_found > 0:\n                    correct_detections += 1\n            \n            results[\"pattern_detection_accuracy\"] = correct_detections / total_samples\n            \n            # Test risk assessment accuracy\n            risk_samples = [\n                {\n                    \"input\": \"Rate risk: 'Standard terms and conditions apply.'\",\n                    \"expected_risk\": \"low\"\n                },\n                {\n                    \"input\": \"Assess: 'Company is not liable for any damages whatsoever.'\",\n                    \"expected_risk\": \"high\"\n                }\n            ]\n            \n            correct_risk_assessments = 0\n            for sample in risk_samples:\n                prediction = await self._generate_single_prediction(\n                    model, tokenizer, sample[\"input\"]\n                )\n                \n                if sample[\"expected_risk\"] in prediction.lower():\n                    correct_risk_assessments += 1\n            \n            results[\"risk_assessment_accuracy\"] = correct_risk_assessments / len(risk_samples)\n            \n            # Calculate clause extraction accuracy\n            results[\"clause_extraction_score\"] = await self._evaluate_clause_extraction(\n                model, tokenizer\n            )\n            \n            return results\n            \n        except Exception as e:\n            logger.error(\"Failed to evaluate legal analysis\", error=str(e))\n            return {}\n    \n    async def _evaluate_marketing_content(\n        self,\n        model: Any,\n        tokenizer: Any,\n        dataset: Dataset\n    ) -> Dict[str, Any]:\n        \"\"\"Evaluate marketing content specific metrics\"\"\"\n        results = {}\n        \n        try:\n            # Test content tone consistency\n            tone_samples = [\n                {\n                    \"input\": \"Write professional email about product launch\",\n                    \"expected_tone\": \"professional\"\n                },\n                {\n                    \"input\": \"Create casual social media post\",\n                    \"expected_tone\": \"casual\"\n                }\n            ]\n            \n            tone_accuracy = 0\n            for sample in tone_samples:\n                prediction = await self._generate_single_prediction(\n                    model, tokenizer, sample[\"input\"]\n                )\n                \n                # Simple tone detection (can be improved with sentiment analysis)\n                if sample[\"expected_tone\"] == \"professional\":\n                    professional_indicators = [\"pleased\", \"announce\", \"excited\", \"opportunity\"]\n                    if any(word in prediction.lower() for word in professional_indicators):\n                        tone_accuracy += 1\n                elif sample[\"expected_tone\"] == \"casual\":\n                    casual_indicators = [\"hey\", \"awesome\", \"cool\", \"check out\"]\n                    if any(word in prediction.lower() for word in casual_indicators):\n                        tone_accuracy += 1\n            \n            results[\"tone_consistency\"] = tone_accuracy / len(tone_samples)\n            \n            # Test call-to-action inclusion\n            cta_samples = [\n                \"Write email with strong call to action\",\n                \"Create ad copy with purchase incentive\"\n            ]\n            \n            cta_inclusion_rate = 0\n            cta_keywords = [\"click\", \"buy\", \"order\", \"subscribe\", \"sign up\", \"learn more\", \"contact\"]\n            \n            for sample in cta_samples:\n                prediction = await self._generate_single_prediction(model, tokenizer, sample)\n                if any(keyword in prediction.lower() for keyword in cta_keywords):\n                    cta_inclusion_rate += 1\n            \n            results[\"cta_inclusion_rate\"] = cta_inclusion_rate / len(cta_samples)\n            \n            # Test brand voice consistency\n            results[\"brand_voice_score\"] = await self._evaluate_brand_voice_consistency(\n                model, tokenizer\n            )\n            \n            return results\n            \n        except Exception as e:\n            logger.error(\"Failed to evaluate marketing content\", error=str(e))\n            return {}\n    \n    async def _evaluate_sales_communication(\n        self,\n        model: Any,\n        tokenizer: Any,\n        dataset: Dataset\n    ) -> Dict[str, Any]:\n        \"\"\"Evaluate sales communication specific metrics\"\"\"\n        results = {}\n        \n        try:\n            # Test personalization accuracy\n            personalization_samples = [\n                {\n                    \"input\": \"Write follow-up email for John from ABC Corp interested in enterprise plan\",\n                    \"expected_elements\": [\"john\", \"abc corp\", \"enterprise\"]\n                }\n            ]\n            \n            personalization_score = 0\n            for sample in personalization_samples:\n                prediction = await self._generate_single_prediction(\n                    model, tokenizer, sample[\"input\"]\n                )\n                \n                elements_found = sum(\n                    1 for element in sample[\"expected_elements\"]\n                    if element.lower() in prediction.lower()\n                )\n                personalization_score += elements_found / len(sample[\"expected_elements\"])\n            \n            results[\"personalization_accuracy\"] = personalization_score / len(personalization_samples)\n            \n            # Test objection handling\n            objection_samples = [\n                \"Respond to: 'Your price is too high'\",\n                \"Handle: 'I need to think about it'\"\n            ]\n            \n            objection_handling_score = 0\n            objection_keywords = [\"value\", \"roi\", \"benefit\", \"understand\", \"consider\"]\n            \n            for sample in objection_samples:\n                prediction = await self._generate_single_prediction(model, tokenizer, sample)\n                if any(keyword in prediction.lower() for keyword in objection_keywords):\n                    objection_handling_score += 1\n            \n            results[\"objection_handling_score\"] = objection_handling_score / len(objection_samples)\n            \n            return results\n            \n        except Exception as e:\n            logger.error(\"Failed to evaluate sales communication\", error=str(e))\n            return {}\n    \n    async def _evaluate_customer_support(\n        self,\n        model: Any,\n        tokenizer: Any,\n        dataset: Dataset\n    ) -> Dict[str, Any]:\n        \"\"\"Evaluate customer support specific metrics\"\"\"\n        results = {}\n        \n        try:\n            # Test empathy and tone\n            support_samples = [\n                \"Respond to angry customer complaint about billing\",\n                \"Help customer with technical issue\"\n            ]\n            \n            empathy_score = 0\n            empathy_keywords = [\"understand\", \"sorry\", \"apologize\", \"help\", \"resolve\"]\n            \n            for sample in support_samples:\n                prediction = await self._generate_single_prediction(model, tokenizer, sample)\n                if any(keyword in prediction.lower() for keyword in empathy_keywords):\n                    empathy_score += 1\n            \n            results[\"empathy_score\"] = empathy_score / len(support_samples)\n            \n            # Test solution accuracy\n            technical_samples = [\n                \"Customer can't log in to account\",\n                \"Product not working as expected\"\n            ]\n            \n            solution_score = 0\n            solution_keywords = [\"reset\", \"check\", \"try\", \"steps\", \"troubleshoot\"]\n            \n            for sample in technical_samples:\n                prediction = await self._generate_single_prediction(model, tokenizer, sample)\n                if any(keyword in prediction.lower() for keyword in solution_keywords):\n                    solution_score += 1\n            \n            results[\"solution_orientation_score\"] = solution_score / len(technical_samples)\n            \n            return results\n            \n        except Exception as e:\n            logger.error(\"Failed to evaluate customer support\", error=str(e))\n            return {}\n    \n    async def _generate_predictions(\n        self,\n        model: Any,\n        tokenizer: Any,\n        dataset: Dataset\n    ) -> Tuple[List[str], List[str]]:\n        \"\"\"Generate predictions for evaluation\"\"\"\n        predictions = []\n        references = []\n        \n        # Limit to 100 samples for evaluation\n        eval_samples = min(100, len(dataset))\n        \n        for i in range(eval_samples):\n            sample = dataset[i]\n            \n            # Extract input and reference\n            input_text = sample.get(\"input\", sample.get(\"text\", \"\"))\n            reference = sample.get(\"output\", sample.get(\"target\", \"\"))\n            \n            if input_text and reference:\n                prediction = await self._generate_single_prediction(model, tokenizer, input_text)\n                predictions.append(prediction)\n                references.append(reference)\n        \n        return predictions, references\n    \n    async def _generate_single_prediction(\n        self,\n        model: Any,\n        tokenizer: Any,\n        input_text: str,\n        max_length: int = 256\n    ) -> str:\n        \"\"\"Generate single prediction from model\"\"\"\n        try:\n            # Tokenize input\n            inputs = tokenizer(\n                input_text,\n                return_tensors=\"pt\",\n                truncation=True,\n                max_length=max_length,\n                padding=True\n            )\n            \n            # Move to device if model is on GPU\n            if next(model.parameters()).is_cuda:\n                inputs = {k: v.cuda() for k, v in inputs.items()}\n            \n            # Generate\n            with torch.no_grad():\n                outputs = model.generate(\n                    **inputs,\n                    max_new_tokens=max_length,\n                    do_sample=True,\n                    temperature=0.7,\n                    top_p=0.9,\n                    pad_token_id=tokenizer.eos_token_id\n                )\n            \n            # Decode prediction\n            prediction = tokenizer.decode(\n                outputs[0][inputs[\"input_ids\"].shape[1]:],\n                skip_special_tokens=True\n            )\n            \n            return prediction.strip()\n            \n        except Exception as e:\n            logger.error(\"Failed to generate prediction\", error=str(e))\n            return \"\"\n    \n    async def _calculate_perplexity(\n        self,\n        model: Any,\n        tokenizer: Any,\n        dataset: Dataset\n    ) -> float:\n        \"\"\"Calculate model perplexity\"\"\"\n        try:\n            total_loss = 0\n            total_tokens = 0\n            \n            # Limit to 50 samples for perplexity calculation\n            eval_samples = min(50, len(dataset))\n            \n            for i in range(eval_samples):\n                sample = dataset[i]\n                text = sample.get(\"text\", sample.get(\"input\", \"\"))\n                \n                if not text:\n                    continue\n                \n                # Tokenize\n                inputs = tokenizer(\n                    text,\n                    return_tensors=\"pt\",\n                    truncation=True,\n                    max_length=512\n                )\n                \n                # Move to device if model is on GPU\n                if next(model.parameters()).is_cuda:\n                    inputs = {k: v.cuda() for k, v in inputs.items()}\n                \n                # Calculate loss\n                with torch.no_grad():\n                    outputs = model(**inputs, labels=inputs[\"input_ids\"])\n                    loss = outputs.loss\n                    \n                    total_loss += loss.item() * inputs[\"input_ids\"].numel()\n                    total_tokens += inputs[\"input_ids\"].numel()\n            \n            if total_tokens > 0:\n                avg_loss = total_loss / total_tokens\n                perplexity = torch.exp(torch.tensor(avg_loss)).item()\n                return perplexity\n            \n            return float('inf')\n            \n        except Exception as e:\n            logger.error(\"Failed to calculate perplexity\", error=str(e))\n            return float('inf')\n    \n    async def _evaluate_clause_extraction(self, model: Any, tokenizer: Any) -> float:\n        \"\"\"Evaluate clause extraction accuracy for legal analysis\"\"\"\n        # Simplified clause extraction test\n        test_cases = [\n            {\n                \"text\": \"Extract the termination clause from this agreement.\",\n                \"expected\": \"termination\"\n            },\n            {\n                \"text\": \"Find liability limitations in this contract.\",\n                \"expected\": \"liability\"\n            }\n        ]\n        \n        correct = 0\n        for case in test_cases:\n            prediction = await self._generate_single_prediction(model, tokenizer, case[\"text\"])\n            if case[\"expected\"] in prediction.lower():\n                correct += 1\n        \n        return correct / len(test_cases)\n    \n    async def _evaluate_brand_voice_consistency(self, model: Any, tokenizer: Any) -> float:\n        \"\"\"Evaluate brand voice consistency for marketing content\"\"\"\n        # Simplified brand voice test\n        brand_samples = [\n            \"Write in our brand voice about innovation\",\n            \"Create content that reflects our values\"\n        ]\n        \n        consistency_score = 0\n        brand_keywords = [\"innovative\", \"cutting-edge\", \"quality\", \"excellence\"]\n        \n        for sample in brand_samples:\n            prediction = await self._generate_single_prediction(model, tokenizer, sample)\n            if any(keyword in prediction.lower() for keyword in brand_keywords):\n                consistency_score += 1\n        \n        return consistency_score / len(brand_samples)\n    \n    async def _update_evaluation_status(self, evaluation_id: str, status: EvaluationStatus):\n        \"\"\"Update evaluation status\"\"\"\n        eval_data = self.active_evaluations[evaluation_id]\n        eval_data[\"status\"] = status.value\n        \n        # Update Redis\n        await self.redis.hset(\n            f\"evaluation:{evaluation_id}\",\n            \"status\",\n            json.dumps(status.value)\n        )"