"""
Model Registry Service for Fine Print AI LoRA Training
Manages trained model metadata, versioning, and storage
"""

import json
import os
import shutil
import hashlib
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, Optional, List
import uuid

from sqlalchemy.ext.asyncio import AsyncSession
import redis.asyncio as aioredis
import boto3
from botocore.exceptions import ClientError

from ..core.config import settings
from ..core.logging_config import get_logger
from ..models.schemas import (
    ModelRegistrationRequest, RegisteredModel, ModelMetadata,
    ModelStatus, BusinessType, ModelFormat
)

logger = get_logger("model_registry")


class ModelRegistryService:
    """Model registry service for managing trained LoRA adapters"""
    
    def __init__(self, db: AsyncSession, redis: aioredis.Redis):
        self.db = db
        self.redis = redis
        self.registry_path = Path(settings.model_registry_local_path)
        self.s3_client = None
        
        # Initialize S3 client if configured
        if settings.model_registry_s3_bucket:
            try:
                self.s3_client = boto3.client('s3')
                logger.info("S3 client initialized for model registry")
            except Exception as e:
                logger.warning("Failed to initialize S3 client", error=str(e))
    
    async def initialize(self):
        """Initialize model registry service"""
        try:
            # Ensure registry directory exists
            self.registry_path.mkdir(parents=True, exist_ok=True)
            
            # Initialize Redis keys
            await self.redis.delete("models:index")
            
            # Rebuild index from existing models
            await self._rebuild_model_index()
            
            logger.info("Model Registry Service initialized successfully")
            
        except Exception as e:
            logger.error("Failed to initialize Model Registry Service", error=str(e))
            raise
    
    async def register_model(self, request: ModelRegistrationRequest) -> str:
        """Register a trained LoRA model"""
        try:
            model_id = request.model_metadata.model_id or str(uuid.uuid4())
            
            # Validate model files exist
            model_path = Path(request.model_path)
            if not model_path.exists():
                raise ValueError(f"Model path does not exist: {request.model_path}")
            
            # Calculate model size and hash
            model_size_mb = await self._calculate_model_size(model_path)
            model_hash = await self._calculate_model_hash(model_path)
            
            # Create model registry entry
            registry_entry = {\n                \"model_id\": model_id,\n                \"model_name\": request.model_metadata.model_name,\n                \"version\": request.model_metadata.version,\n                \"business_type\": request.model_metadata.business_type.value,\n                \"base_model\": request.model_metadata.base_model,\n                \"status\": ModelStatus.READY.value,\n                \"description\": request.model_metadata.description,\n                \"tags\": request.model_metadata.tags,\n                \"author\": request.model_metadata.author,\n                \"license\": request.model_metadata.license,\n                \"training_job_id\": request.training_job_id,\n                \"evaluation_metrics\": request.evaluation_metrics,\n                \"model_format\": request.model_format.value,\n                \"model_size_mb\": model_size_mb,\n                \"model_hash\": model_hash,\n                \"training_config\": request.training_config,\n                \"created_at\": datetime.utcnow().isoformat(),\n                \"updated_at\": datetime.utcnow().isoformat(),\n                \"local_path\": None,\n                \"s3_path\": None,\n                \"download_count\": 0,\n                \"last_accessed\": None\n            }\n            \n            # Store model files\n            local_model_path = await self._store_model_locally(model_id, model_path)\n            registry_entry[\"local_path\"] = str(local_model_path)\n            \n            # Upload to S3 if configured\n            if self.s3_client and settings.model_registry_s3_bucket:\n                s3_path = await self._upload_to_s3(model_id, model_path)\n                registry_entry[\"s3_path\"] = s3_path\n            \n            # Store in Redis\n            await self.redis.hset(\n                f\"model:{model_id}\",\n                mapping={k: json.dumps(v) for k, v in registry_entry.items()}\n            )\n            \n            # Add to model index\n            await self.redis.zadd(\n                \"models:index\",\n                {model_id: datetime.utcnow().timestamp()}\n            )\n            \n            # Add to business type index\n            await self.redis.sadd(\n                f\"models:business_type:{request.model_metadata.business_type.value}\",\n                model_id\n            )\n            \n            # Add to base model index\n            await self.redis.sadd(\n                f\"models:base_model:{request.model_metadata.base_model}\",\n                model_id\n            )\n            \n            logger.info(\n                \"Model registered successfully\",\n                model_id=model_id,\n                model_name=request.model_metadata.model_name,\n                business_type=request.model_metadata.business_type.value,\n                size_mb=model_size_mb\n            )\n            \n            return model_id\n            \n        except Exception as e:\n            logger.error(\"Failed to register model\", error=str(e))\n            raise\n    \n    async def get_model(self, model_id: str) -> RegisteredModel:\n        \"\"\"Get model details by ID\"\"\"\n        try:\n            # Get from Redis\n            model_data = await self.redis.hgetall(f\"model:{model_id}\")\n            if not model_data:\n                raise ValueError(f\"Model {model_id} not found\")\n            \n            # Parse data\n            parsed_data = {k: json.loads(v) for k, v in model_data.items()}\n            \n            # Update last accessed\n            parsed_data[\"last_accessed\"] = datetime.utcnow().isoformat()\n            await self.redis.hset(\n                f\"model:{model_id}\",\n                \"last_accessed\",\n                json.dumps(parsed_data[\"last_accessed\"])\n            )\n            \n            # Create model metadata object\n            metadata = ModelMetadata(\n                model_id=parsed_data[\"model_id\"],\n                model_name=parsed_data[\"model_name\"],\n                business_type=BusinessType(parsed_data[\"business_type\"]),\n                base_model=parsed_data[\"base_model\"],\n                version=parsed_data[\"version\"],\n                description=parsed_data.get(\"description\"),\n                tags=parsed_data.get(\"tags\", []),\n                author=parsed_data.get(\"author\"),\n                license=parsed_data.get(\"license\"),\n                status=ModelStatus(parsed_data[\"status\"]),\n                created_at=datetime.fromisoformat(parsed_data[\"created_at\"]),\n                updated_at=datetime.fromisoformat(parsed_data[\"updated_at\"])\n            )\n            \n            return RegisteredModel(\n                model_id=parsed_data[\"model_id\"],\n                model_name=parsed_data[\"model_name\"],\n                version=parsed_data[\"version\"],\n                status=ModelStatus(parsed_data[\"status\"]),\n                metadata=metadata,\n                created_at=datetime.fromisoformat(parsed_data[\"created_at\"]),\n                updated_at=datetime.fromisoformat(parsed_data[\"updated_at\"]),\n                download_url=parsed_data.get(\"s3_path\") or parsed_data.get(\"local_path\")\n            )\n            \n        except Exception as e:\n            logger.error(\"Failed to get model\", model_id=model_id, error=str(e))\n            raise\n    \n    async def list_models(\n        self,\n        limit: int = 50,\n        offset: int = 0,\n        business_type: Optional[str] = None,\n        status: Optional[str] = None,\n        base_model: Optional[str] = None\n    ) -> List[RegisteredModel]:\n        \"\"\"List registered models with filtering\"\"\"\n        try:\n            model_ids = []\n            \n            if business_type:\n                # Get models by business type\n                model_ids = await self.redis.smembers(f\"models:business_type:{business_type}\")\n            elif base_model:\n                # Get models by base model\n                model_ids = await self.redis.smembers(f\"models:base_model:{base_model}\")\n            else:\n                # Get all models from index (sorted by creation time)\n                model_ids = await self.redis.zrevrange(\"models:index\", offset, offset + limit - 1)\n            \n            models = []\n            for model_id in model_ids[offset:offset+limit]:\n                try:\n                    model = await self.get_model(model_id)\n                    \n                    # Apply status filter\n                    if status and model.status.value != status:\n                        continue\n                    \n                    models.append(model)\n                    \n                except Exception as e:\n                    logger.warning(\"Failed to get model details\", model_id=model_id, error=str(e))\n                    continue\n            \n            return models\n            \n        except Exception as e:\n            logger.error(\"Failed to list models\", error=str(e))\n            raise\n    \n    async def delete_model(self, model_id: str) -> bool:\n        \"\"\"Delete a model from registry\"\"\"\n        try:\n            # Get model data\n            model_data = await self.redis.hgetall(f\"model:{model_id}\")\n            if not model_data:\n                raise ValueError(f\"Model {model_id} not found\")\n            \n            parsed_data = {k: json.loads(v) for k, v in model_data.items()}\n            \n            # Remove local files\n            if parsed_data.get(\"local_path\"):\n                local_path = Path(parsed_data[\"local_path\"])\n                if local_path.exists():\n                    shutil.rmtree(local_path, ignore_errors=True)\n            \n            # Remove from S3\n            if self.s3_client and parsed_data.get(\"s3_path\"):\n                try:\n                    self.s3_client.delete_object(\n                        Bucket=settings.model_registry_s3_bucket,\n                        Key=parsed_data[\"s3_path\"]\n                    )\n                except ClientError as e:\n                    logger.warning(\"Failed to delete S3 object\", error=str(e))\n            \n            # Remove from Redis\n            await self.redis.delete(f\"model:{model_id}\")\n            await self.redis.zrem(\"models:index\", model_id)\n            \n            # Remove from indexes\n            business_type = parsed_data.get(\"business_type\")\n            if business_type:\n                await self.redis.srem(f\"models:business_type:{business_type}\", model_id)\n            \n            base_model = parsed_data.get(\"base_model\")\n            if base_model:\n                await self.redis.srem(f\"models:base_model:{base_model}\", model_id)\n            \n            logger.info(\"Model deleted successfully\", model_id=model_id)\n            return True\n            \n        except Exception as e:\n            logger.error(\"Failed to delete model\", model_id=model_id, error=str(e))\n            raise\n    \n    async def update_model_status(self, model_id: str, status: ModelStatus) -> bool:\n        \"\"\"Update model status\"\"\"\n        try:\n            # Check if model exists\n            model_exists = await self.redis.exists(f\"model:{model_id}\")\n            if not model_exists:\n                raise ValueError(f\"Model {model_id} not found\")\n            \n            # Update status and timestamp\n            await self.redis.hset(\n                f\"model:{model_id}\",\n                mapping={\n                    \"status\": json.dumps(status.value),\n                    \"updated_at\": json.dumps(datetime.utcnow().isoformat())\n                }\n            )\n            \n            logger.info(\"Model status updated\", model_id=model_id, status=status.value)\n            return True\n            \n        except Exception as e:\n            logger.error(\"Failed to update model status\", model_id=model_id, error=str(e))\n            raise\n    \n    async def get_model_download_url(self, model_id: str) -> str:\n        \"\"\"Get model download URL\"\"\"\n        try:\n            model_data = await self.redis.hgetall(f\"model:{model_id}\")\n            if not model_data:\n                raise ValueError(f\"Model {model_id} not found\")\n            \n            parsed_data = {k: json.loads(v) for k, v in model_data.items()}\n            \n            # Increment download count\n            current_count = parsed_data.get(\"download_count\", 0)\n            await self.redis.hset(\n                f\"model:{model_id}\",\n                \"download_count\",\n                json.dumps(current_count + 1)\n            )\n            \n            # Return S3 URL if available, otherwise local path\n            if parsed_data.get(\"s3_path\") and self.s3_client:\n                # Generate pre-signed URL for S3\n                try:\n                    url = self.s3_client.generate_presigned_url(\n                        'get_object',\n                        Params={\n                            'Bucket': settings.model_registry_s3_bucket,\n                            'Key': parsed_data[\"s3_path\"]\n                        },\n                        ExpiresIn=3600  # 1 hour\n                    )\n                    return url\n                except ClientError as e:\n                    logger.warning(\"Failed to generate S3 presigned URL\", error=str(e))\n            \n            # Return local path\n            return parsed_data.get(\"local_path\", \"\")\n            \n        except Exception as e:\n            logger.error(\"Failed to get download URL\", model_id=model_id, error=str(e))\n            raise\n    \n    async def search_models(self, query: str, limit: int = 20) -> List[RegisteredModel]:\n        \"\"\"Search models by name, description, or tags\"\"\"\n        try:\n            # Get all model IDs\n            all_model_ids = await self.redis.zrevrange(\"models:index\", 0, -1)\n            \n            matching_models = []\n            query_lower = query.lower()\n            \n            for model_id in all_model_ids:\n                if len(matching_models) >= limit:\n                    break\n                \n                try:\n                    model = await self.get_model(model_id)\n                    \n                    # Check if query matches name, description, or tags\n                    if (\n                        query_lower in model.model_name.lower() or\n                        (model.metadata.description and query_lower in model.metadata.description.lower()) or\n                        any(query_lower in tag.lower() for tag in model.metadata.tags)\n                    ):\n                        matching_models.append(model)\n                        \n                except Exception as e:\n                    logger.warning(\"Failed to search model\", model_id=model_id, error=str(e))\n                    continue\n            \n            return matching_models\n            \n        except Exception as e:\n            logger.error(\"Failed to search models\", query=query, error=str(e))\n            raise\n    \n    async def get_model_statistics(self) -> Dict[str, Any]:\n        \"\"\"Get model registry statistics\"\"\"\n        try:\n            # Get total model count\n            total_models = await self.redis.zcard(\"models:index\")\n            \n            # Get counts by business type\n            business_type_counts = {}\n            for business_type in settings.business_types:\n                count = await self.redis.scard(f\"models:business_type:{business_type}\")\n                business_type_counts[business_type] = count\n            \n            # Get counts by status\n            status_counts = {\n                \"training\": 0,\n                \"ready\": 0,\n                \"deployed\": 0,\n                \"deprecated\": 0,\n                \"failed\": 0\n            }\n            \n            # This is inefficient for large datasets, but works for now\n            all_model_ids = await self.redis.zrevrange(\"models:index\", 0, -1)\n            for model_id in all_model_ids:\n                try:\n                    model_data = await self.redis.hgetall(f\"model:{model_id}\")\n                    if model_data:\n                        status = json.loads(model_data.get(\"status\", '\"unknown\"'))\n                        if status in status_counts:\n                            status_counts[status] += 1\n                except Exception:\n                    continue\n            \n            # Calculate total storage used\n            total_storage_mb = 0\n            for model_id in all_model_ids:\n                try:\n                    model_data = await self.redis.hgetall(f\"model:{model_id}\")\n                    if model_data and \"model_size_mb\" in model_data:\n                        size = json.loads(model_data[\"model_size_mb\"])\n                        total_storage_mb += size\n                except Exception:\n                    continue\n            \n            return {\n                \"total_models\": total_models,\n                \"business_type_distribution\": business_type_counts,\n                \"status_distribution\": status_counts,\n                \"total_storage_mb\": total_storage_mb,\n                \"storage_backend\": \"s3\" if self.s3_client else \"local\",\n                \"registry_path\": str(self.registry_path)\n            }\n            \n        except Exception as e:\n            logger.error(\"Failed to get model statistics\", error=str(e))\n            raise\n    \n    async def health_check(self) -> bool:\n        \"\"\"Check model registry health\"\"\"\n        try:\n            # Check Redis connectivity\n            await self.redis.ping()\n            \n            # Check local registry path\n            if not self.registry_path.exists():\n                return False\n            \n            # Check S3 connectivity if configured\n            if self.s3_client and settings.model_registry_s3_bucket:\n                try:\n                    self.s3_client.head_bucket(Bucket=settings.model_registry_s3_bucket)\n                except ClientError:\n                    return False\n            \n            return True\n            \n        except Exception as e:\n            logger.error(\"Model registry health check failed\", error=str(e))\n            return False\n    \n    async def cleanup(self):\n        \"\"\"Cleanup model registry service\"\"\"\n        try:\n            logger.info(\"Model Registry Service cleaned up\")\n        except Exception as e:\n            logger.error(\"Error during cleanup\", error=str(e))\n    \n    async def _store_model_locally(self, model_id: str, source_path: Path) -> Path:\n        \"\"\"Store model files locally\"\"\"\n        target_path = self.registry_path / model_id\n        target_path.mkdir(parents=True, exist_ok=True)\n        \n        if source_path.is_file():\n            # Single file\n            shutil.copy2(source_path, target_path / source_path.name)\n        else:\n            # Directory\n            shutil.copytree(source_path, target_path, dirs_exist_ok=True)\n        \n        return target_path\n    \n    async def _upload_to_s3(self, model_id: str, model_path: Path) -> str:\n        \"\"\"Upload model to S3\"\"\"\n        if not self.s3_client:\n            raise ValueError(\"S3 client not configured\")\n        \n        s3_key = f\"models/{model_id}/{model_path.name}\"\n        \n        try:\n            if model_path.is_file():\n                # Upload single file\n                self.s3_client.upload_file(\n                    str(model_path),\n                    settings.model_registry_s3_bucket,\n                    s3_key\n                )\n            else:\n                # Upload directory as zip\n                import zipfile\n                zip_path = model_path.parent / f\"{model_id}.zip\"\n                \n                with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n                    for file_path in model_path.rglob('*'):\n                        if file_path.is_file():\n                            zipf.write(file_path, file_path.relative_to(model_path))\n                \n                self.s3_client.upload_file(\n                    str(zip_path),\n                    settings.model_registry_s3_bucket,\n                    f\"models/{model_id}.zip\"\n                )\n                \n                # Clean up local zip\n                zip_path.unlink()\n                s3_key = f\"models/{model_id}.zip\"\n            \n            return s3_key\n            \n        except ClientError as e:\n            logger.error(\"Failed to upload to S3\", model_id=model_id, error=str(e))\n            raise\n    \n    async def _calculate_model_size(self, model_path: Path) -> float:\n        \"\"\"Calculate model size in MB\"\"\"\n        total_size = 0\n        \n        if model_path.is_file():\n            total_size = model_path.stat().st_size\n        else:\n            for file_path in model_path.rglob('*'):\n                if file_path.is_file():\n                    total_size += file_path.stat().st_size\n        \n        return total_size / (1024 * 1024)  # Convert to MB\n    \n    async def _calculate_model_hash(self, model_path: Path) -> str:\n        \"\"\"Calculate model hash for integrity verification\"\"\"\n        hasher = hashlib.sha256()\n        \n        if model_path.is_file():\n            with open(model_path, 'rb') as f:\n                for chunk in iter(lambda: f.read(4096), b\"\"):\n                    hasher.update(chunk)\n        else:\n            # Hash all files in directory\n            for file_path in sorted(model_path.rglob('*')):\n                if file_path.is_file():\n                    hasher.update(str(file_path.relative_to(model_path)).encode())\n                    with open(file_path, 'rb') as f:\n                        for chunk in iter(lambda: f.read(4096), b\"\"):\n                            hasher.update(chunk)\n        \n        return hasher.hexdigest()\n    \n    async def _rebuild_model_index(self):\n        \"\"\"Rebuild model index from existing models\"\"\"\n        try:\n            # Get all model keys from Redis\n            model_keys = await self.redis.keys(\"model:*\")\n            \n            for key in model_keys:\n                model_id = key.split(\":\", 1)[1]\n                model_data = await self.redis.hgetall(key)\n                \n                if model_data and \"created_at\" in model_data:\n                    created_at = json.loads(model_data[\"created_at\"])\n                    timestamp = datetime.fromisoformat(created_at).timestamp()\n                    \n                    # Add to main index\n                    await self.redis.zadd(\"models:index\", {model_id: timestamp})\n                    \n                    # Add to business type index\n                    if \"business_type\" in model_data:\n                        business_type = json.loads(model_data[\"business_type\"])\n                        await self.redis.sadd(f\"models:business_type:{business_type}\", model_id)\n                    \n                    # Add to base model index\n                    if \"base_model\" in model_data:\n                        base_model = json.loads(model_data[\"base_model\"])\n                        await self.redis.sadd(f\"models:base_model:{base_model}\", model_id)\n            \n            logger.info(f\"Rebuilt model index with {len(model_keys)} models\")\n            \n        except Exception as e:\n            logger.error(\"Failed to rebuild model index\", error=str(e))"